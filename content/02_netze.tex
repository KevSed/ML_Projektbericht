\chapter{Convolutional Neural Networks}\label{sec:netze}

Zur Klassifizierung von Bildern haben sich in der Vergangenheit so genannte
\textit{Convolutional Neural Networks} (CNN) als sehr effektiv bewiesen \cite{goodfellow}.
Diese wenden, wie im Namen impliziert, die mathematische Operation der Faltung
(oder in den meisten Fällen der Kreuzkorrelation) zwischen mindestens zwei
\textit{layern} des Netzes an.
Dazu wird ein Kernel definiert, welcher deutlich kleiner als die Eingabe ist.
Dieser Kernel rastert die Eingabe ab und erzeugt dabei die Ausgabe lokal auf
kleineren subsamples der Eingabe. Dabei wird also ein set linearer Aktivierungen
erzeugt.
Diese speziellen neuronalen Netze unterscheiden sich von herkömmlichen Netzen
allgemein in 3 Punkten:
%
\begin{enumerate}
  \item \textbf{Wechselwirkungen}: Anstatt wie bei vollständig vernetzten
  Netzen durch eine einfache Matrixmultiplikation alle Knoten zwischen zwei
  Lagen wechselwirken zu lassen, werden durch kleine Kernelgrößen
  Eigenschaften auf deutlich kleineren subsamples der Eingabe bestimmt. Dies dient
  z.B. der Erkennung von Strukturen und Eigenschaften der zu erkennenden Dinge.
  \item \textbf{Parameter}: In herkömmlichen Netzen werden die vom Netz
  generierten Gewichte exakt einmal genutzt, nämlich wenn die Ausgabe der
  besagten Lage generiert wird. CNN hingegen teilen sich diese Parameter mit
  verschiedenen Knoten. Gewichte eines Knotens sind also an Gewichte eines
  Anderen gebunden, da derselbe Kernel überall angewandt wird.
  \item \textbf{Equivarainz}: Die Struktur von CNN verleiht diesen eine
  Eigenschaft, die Equivarainz genannt wird. Das heißt, dass sich
  Verschiebungen der Eingabe genauso in der Ausgabe widerspiegeln. Dies ermöglicht
  CNN, bestimmte Eigenschaften in z.B. einem Bild an verschiedenen Orten zu erkennen
  und deren gewichte zu teilen.
\end{enumerate}
%
Diese Unterschiede stellen bereits deutlich heraus, was CNN so geeignet für die
Erkennung von Bildern macht. Die Verbindung von mehreren \textit{convolutional
layern} ermöglicht so die sukzessive Erkennung von Strukturen in Bildern. \\
Typisch für Netze mit der Aufgabe der Bilderkennung ist also eine oben
beschriebene Faltungsoperation, gefolgt von einer nichtlinearen Aktivierung.
Der Output dieser Faltungslagen wird in einem zweiten Schritt in einem so
genannten \textbf{pooling layer} (PL) verarbeitet. Dieser nimmt im Prinzip eine
Vergröberung vor. Der hier verwendete \textit{max pooling layer} z.B. ersetzt
die Eingabe in einem bestimmten, kleinen Bereich durch das Maximum. So wird das
Gelernte unabhängiger von kleinen Änderungen gemacht, da die Ausgabe des PL sich
unter kleinen Verschiebungen der Eingabe nur geringfügig ändern. Dieser Schritt
stellt eine wichtige Komponente eines funktionierenden Netzes dar, wenn es
nicht nur wichtig ist, wo sich ein bestimmtes Merkmal befindet, sondern
ob es sich in dem Bild befindet.
Außerdem spielen PL eine wichtige Rolle bei der Verarbeitung von Daten
variabler Dimension.
